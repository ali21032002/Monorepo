import os
import sys
import httpx
from typing import Any, Dict, Optional

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse

from . import config
from .models import ExtractionRequest, ExtractionResponse, SchemasResponse, MultiModelRequest, MultiModelResponse, DomainsResponse, ModelAnalysis, ChatRequest, ChatResponse, SpeechToTextRequest, SpeechToTextResponse, ChartData, ChartDataset
from .file_extract import extract_text_from_file

# Add shared package to sys.path
CURRENT_DIR = os.path.dirname(__file__)
MONOREPO_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, "..", ".."))
SHARED_DIR = os.path.join(MONOREPO_ROOT, "shared")
if SHARED_DIR not in sys.path:
	sys.path.insert(0, SHARED_DIR)

# Import shared langextract after adjusting path
from langextract import run_extraction, run_multi_model_analysis, generate_html_report, list_schemas  # type: ignore
from langextract.ollama_backend import chat_conversational  # type: ignore

app = FastAPI(title="LangExtract Service", version="0.2.0")

app.add_middleware(
	CORSMiddleware,
	allow_origins=config.ALLOW_ORIGINS,
	allow_credentials=config.ALLOW_CREDENTIALS,
	allow_methods=config.ALLOW_METHODS,
	allow_headers=config.ALLOW_HEADERS,
)


@app.get("/api/health")
def health() -> Dict[str, Any]:
	return {
		"status": "ok",
		"ollama_host": config.OLLAMA_HOST,
		"model": config.OLLAMA_MODEL,
	}


@app.get("/api/schemas", response_model=SchemasResponse)
def schemas() -> SchemasResponse:
	return SchemasResponse(schemas=list_schemas())


@app.get("/api/domains", response_model=DomainsResponse)
def domains() -> DomainsResponse:
	return DomainsResponse(domains=["general", "legal", "medical", "police"])


@app.get("/api/models")
def get_ollama_models() -> Dict[str, Any]:
	"""Get list of available Ollama models"""
	try:
		import subprocess
		import json
		
		# Try ollama list with JSON first
		result = subprocess.run(
			["ollama", "list", "--json"], 
			capture_output=True, 
			text=True, 
			timeout=10
		)
		
		if result.returncode == 0:
			try:
				models_data = json.loads(result.stdout)
				models = []
				
				# Extract model names
				if "models" in models_data:
					for model in models_data["models"]:
						if "name" in model:
							models.append(model["name"])
				
				return {
					"status": "success",
					"models": models,
					"count": len(models)
				}
			except json.JSONDecodeError:
				pass  # Fall through to text parsing
		
		# Try regular ollama list (text format)
		result = subprocess.run(
			["ollama", "list"], 
			capture_output=True, 
			text=True, 
			timeout=10
		)
		
		if result.returncode == 0:
			# Parse text output
			lines = result.stdout.strip().split('\n')
			models = []
			for line in lines[1:]:  # Skip header line
				if line.strip():
					parts = line.split()
					if parts:
						model_name = parts[0]  # First column is model name
						# Filter out system models and keep only main models
						if not model_name.startswith('.') and ':' in model_name:
							models.append(model_name)
			
			return {
				"status": "success", 
				"models": models,
				"count": len(models)
			}
		else:
			# Return some default models if ollama command fails
			default_models = ["gemma3:4b", "qwen2.5:7b", "gemma2:9b", "llama3:8b"]
			return {
				"status": "fallback",
				"models": default_models,
				"count": len(default_models),
				"error": "Could not connect to Ollama, showing common models"
			}
			
	except Exception as e:
		# Return fallback models in case of any error
		default_models = ["gemma3:4b", "qwen2.5:7b", "gemma2:9b", "llama3:8b"]
		return {
			"status": "error",
			"models": default_models,
			"count": len(default_models),
			"error": str(e)
		}


@app.post("/api/extract", response_model=ExtractionResponse)
def extract(req: ExtractionRequest) -> ExtractionResponse:
	if not req.text or not req.text.strip():
		raise HTTPException(status_code=400, detail="'text' is required")

	language = (req.language or "fa").lower()
	model_name = req.model or config.OLLAMA_MODEL
	temperature = req.temperature if req.temperature is not None else config.TEMPERATURE
	max_output_tokens = req.max_output_tokens if req.max_output_tokens is not None else config.MAX_OUTPUT_TOKENS

	# Check if user is asking about the AI assistant
	ai_question_keywords = [
		'ÿ™Ÿà ⁄©€å Ÿáÿ≥ÿ™€å', 'ÿ™Ÿà ⁄©ÿ¨ÿß ÿ™Ÿàÿ≥ÿπŸá Ÿæ€åÿØÿß ⁄©ÿ±ÿØ€å', '⁄ÜŸá ⁄©ÿ≥€å ŸÜŸàÿ¥ÿ™Ÿá ÿßÿ™', '⁄ÜŸá ⁄©ÿ≥€å ÿ™Ÿàÿ≥ÿπŸá ÿØÿßÿØŸá ÿßÿ™', 'ÿπŸÑ€å ÿ≥ŸÑ€åŸÖ€å ⁄©€åŸáÿü',
		'⁄©ÿ¨ÿß ÿ¢ŸÖŸàÿ≤ÿ¥ ÿØ€åÿØŸá ÿß€å', 'ÿ™Ÿàÿ≥ÿπŸá ÿØŸáŸÜÿØŸá ÿ™Ÿà ⁄©€åÿ≥ÿ™', 'ŸÜŸà€åÿ≥ŸÜÿØŸá ÿ™Ÿà ⁄©€åÿ≥ÿ™', '⁄ÜŸá ⁄©ÿ≥€å ÿ™Ÿà ÿ±ÿß ÿ≥ÿßÿÆÿ™Ÿá','ŸÜŸà€åÿ≥ŸÜÿØŸá ÿ™Ÿà ⁄ÜŸá ⁄©ÿ≥€å ÿßÿ≥ÿ™' ,
		'who are you', 'who created you', 'who developed you', 'who wrote you',
		'where were you developed', 'where were you trained'
	]
	is_ai_question = any(keyword in req.text.lower() for keyword in ai_question_keywords)
	
	if is_ai_question:
		ai_response = """ŸÖŸÜ ÿØÿ≥ÿ™€åÿßÿ± ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å Ÿáÿ≥ÿ™ŸÖ ⁄©Ÿá ÿØÿ± ŸÖÿ±⁄©ÿ≤ ŸÖÿØ€åÿ±€åÿ™ Ÿà ÿ™ÿ≠ŸÑ€åŸÑ ÿØÿßÿØŸá ŸÅÿ±ÿßÿ¨ÿß Ÿà ÿßÿØÿßÿ±Ÿá ŸÖŸáŸÜÿØÿ≥€å ÿØÿßÿØŸá ÿ™Ÿàÿ≥ÿπŸá ÿØÿßÿØŸá ÿ¥ÿØŸá Ÿà ÿ¢ŸÖŸàÿ≤ÿ¥ ÿØ€åÿØŸá‚ÄåÿßŸÖ. ÿ™Ÿàÿ≥ÿπŸá‚ÄåÿØŸáŸÜÿØŸá ŸÖŸÜÿå ÿ≥ÿ±ŸáŸÜ⁄Ø ŸÖŸáŸÜÿØÿ≥ ÿπŸÑ€å ÿ≥ŸÑ€åŸÖ€å ÿßÿ≥ÿ™ Ÿà ÿ¢ŸÖÿßÿØŸá‚ÄåÿßŸÖ ÿ™ÿß ÿ¥ŸÖÿß ÿ±ÿß ÿ±ÿßŸáŸÜŸÖÿß€å€å ⁄©ŸÜŸÖ."""
		return ExtractionResponse(
			text=req.text,
			language=language,
			model=model_name,
			entities=[{
				"text": ai_response,
				"label": "AI_Response",
				"start": 0,
				"end": len(ai_response),
				"confidence": 1.0
			}],
			relationships=[]
		)

	result = run_extraction(
		text=req.text,
		language=language,
		schema=req.schema or "general",
		domain="general",  # Add domain parameter
		examples=[e.model_dump() for e in (req.examples or [])],
		model=model_name,
		temperature=temperature,
		max_output_tokens=max_output_tokens,
		request_timeout_seconds=config.REQUEST_TIMEOUT_SECONDS,
		num_ctx=config.NUM_CTX,
		max_input_chars=config.MAX_INPUT_CHARS,
		chunk_overlap_chars=config.CHUNK_OVERLAP_CHARS,
		max_chunks=config.MAX_CHUNKS,
	)

	return ExtractionResponse(
		text=req.text,
		language=language,
		model=model_name,
		entities=result.get("entities", []),
		relationships=result.get("relationships", []),
	)


@app.post("/api/extract_file", response_model=ExtractionResponse)
async def extract_file(
	file: UploadFile = File(...),
	language: str = Form("fa"),
	schema: str = Form("general"),
	model: Optional[str] = Form(None),
	temperature: Optional[float] = Form(None),
	max_output_tokens: Optional[int] = Form(None),
) -> ExtractionResponse:
	data = await file.read()
	text = extract_text_from_file(file.filename, data)
	if not text or not text.strip():
		raise HTTPException(status_code=400, detail="File is empty or unsupported format")

	language = (language or "fa").lower()
	model_name = model or config.OLLAMA_MODEL
	temperature = temperature if temperature is not None else config.TEMPERATURE
	max_output_tokens = max_output_tokens if max_output_tokens is not None else config.MAX_OUTPUT_TOKENS

	# Check if user is asking about the AI assistant
	ai_question_keywords = [
		'ÿ™Ÿà ⁄©€å Ÿáÿ≥ÿ™€å', 'ÿ™Ÿà ⁄©ÿ¨ÿß ÿ™Ÿàÿ≥ÿπŸá Ÿæ€åÿØÿß ⁄©ÿ±ÿØ€å', '⁄ÜŸá ⁄©ÿ≥€å ŸÜŸàÿ¥ÿ™Ÿá ÿßÿ™', '⁄ÜŸá ⁄©ÿ≥€å ÿ™Ÿàÿ≥ÿπŸá ÿØÿßÿØŸá ÿßÿ™',
		'⁄©ÿ¨ÿß ÿ¢ŸÖŸàÿ≤ÿ¥ ÿØ€åÿØŸá ÿß€å', 'ÿ™Ÿàÿ≥ÿπŸá ÿØŸáŸÜÿØŸá ÿ™Ÿà ⁄©€åÿ≥ÿ™', 'ŸÜŸà€åÿ≥ŸÜÿØŸá ÿ™Ÿà ⁄©€åÿ≥ÿ™', '⁄ÜŸá ⁄©ÿ≥€å ÿ™Ÿà ÿ±ÿß ÿ≥ÿßÿÆÿ™Ÿá',
		'who are you', 'who created you', 'who developed you', 'who wrote you',
		'where were you developed', 'where were you trained'
	]
	is_ai_question = any(keyword in text.lower() for keyword in ai_question_keywords)
	
	if is_ai_question:
		ai_response = """ŸÖŸÜ ÿØÿ≥ÿ™€åÿßÿ± ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å Ÿáÿ≥ÿ™ŸÖ ⁄©Ÿá ÿØÿ± ŸÖÿ±⁄©ÿ≤ ŸÖÿØ€åÿ±€åÿ™ Ÿà ÿ™ÿ≠ŸÑ€åŸÑ ÿØÿßÿØŸá ŸÅÿ±ÿßÿ¨ÿß Ÿà ÿßÿØÿßÿ±Ÿá ŸÖŸáŸÜÿØÿ≥€å ÿØÿßÿØŸá ÿ™Ÿàÿ≥ÿπŸá ÿØÿßÿØŸá ÿ¥ÿØŸá Ÿà ÿ¢ŸÖŸàÿ≤ÿ¥ ÿØ€åÿØŸá‚ÄåÿßŸÖ. ÿ™Ÿàÿ≥ÿπŸá‚ÄåÿØŸáŸÜÿØŸá ŸÖŸÜÿå ÿ≥ÿ±ŸáŸÜ⁄Ø ŸÖŸáŸÜÿØÿ≥ ÿπŸÑ€å ÿ≥ŸÑ€åŸÖ€å ÿßÿ≥ÿ™ Ÿà ÿ¢ŸÖÿßÿØŸá‚ÄåÿßŸÖ ÿ™ÿß ÿ¥ŸÖÿß ÿ±ÿß ÿ±ÿßŸáŸÜŸÖÿß€å€å ⁄©ŸÜŸÖ."""
		return ExtractionResponse(
			text=text,
			language=language,
			model=model_name,
			entities=[{
				"text": ai_response,
				"label": "AI_Response",
				"start": 0,
				"end": len(ai_response),
				"confidence": 1.0
			}],
			relationships=[]
		)

	result = run_extraction(
		text=text,
		language=language,
		schema=schema or "general",
		domain="general",  # Add domain parameter
		examples=[],
		model=model_name,
		temperature=temperature,
		max_output_tokens=max_output_tokens,
		request_timeout_seconds=config.REQUEST_TIMEOUT_SECONDS,
		num_ctx=config.NUM_CTX,
		max_input_chars=config.MAX_INPUT_CHARS,
		chunk_overlap_chars=config.CHUNK_OVERLAP_CHARS,
		max_chunks=config.MAX_CHUNKS,
	)

	return ExtractionResponse(
		text=text,
		language=language,
		model=model_name,
		entities=result.get("entities", []),
		relationships=result.get("relationships", []),
	)


@app.post("/api/report", response_class=HTMLResponse)
def report(req: ExtractionRequest) -> HTMLResponse:
	if not req.text or not req.text.strip():
		raise HTTPException(status_code=400, detail="'text' is required")

	language = (req.language or "fa").lower()
	model_name = req.model or config.OLLAMA_MODEL
	temperature = req.temperature if req.temperature is not None else config.TEMPERATURE
	max_output_tokens = req.max_output_tokens if req.max_output_tokens is not None else config.MAX_OUTPUT_TOKENS

	result = run_extraction(
		text=req.text,
		language=language,
		schema=req.schema or "general",
		domain="general",  # Add domain parameter
		examples=[e.model_dump() for e in (req.examples or [])],
		model=model_name,
		temperature=temperature,
		max_output_tokens=max_output_tokens,
		request_timeout_seconds=config.REQUEST_TIMEOUT_SECONDS,
		num_ctx=config.NUM_CTX,
		max_input_chars=config.MAX_INPUT_CHARS,
		chunk_overlap_chars=config.CHUNK_OVERLAP_CHARS,
		max_chunks=config.MAX_CHUNKS,
	)

	html = generate_html_report(source_text=req.text, extraction=result, language=language, model=model_name)
	return HTMLResponse(content=html)


@app.post("/api/multi_extract", response_model=MultiModelResponse)
def multi_extract(req: MultiModelRequest) -> MultiModelResponse:
	if not req.text or not req.text.strip():
		raise HTTPException(status_code=400, detail="'text' is required")

	language = (req.language or "fa").lower()
	domain = req.domain or "general"
	temperature = req.temperature if req.temperature is not None else config.TEMPERATURE
	max_output_tokens = req.max_output_tokens if req.max_output_tokens is not None else config.MAX_OUTPUT_TOKENS

	# Check if user is asking about the AI assistant
	ai_question_keywords = [
		'ÿ™Ÿà ⁄©€å Ÿáÿ≥ÿ™€å', 'ÿ™Ÿà ⁄©ÿ¨ÿß ÿ™Ÿàÿ≥ÿπŸá Ÿæ€åÿØÿß ⁄©ÿ±ÿØ€å', '⁄ÜŸá ⁄©ÿ≥€å ŸÜŸàÿ¥ÿ™Ÿá ÿßÿ™', '⁄ÜŸá ⁄©ÿ≥€å ÿ™Ÿàÿ≥ÿπŸá ÿØÿßÿØŸá ÿßÿ™',
		'⁄©ÿ¨ÿß ÿ¢ŸÖŸàÿ≤ÿ¥ ÿØ€åÿØŸá ÿß€å', 'ÿ™Ÿàÿ≥ÿπŸá ÿØŸáŸÜÿØŸá ÿ™Ÿà ⁄©€åÿ≥ÿ™', 'ŸÜŸà€åÿ≥ŸÜÿØŸá ÿ™Ÿà ⁄©€åÿ≥ÿ™', '⁄ÜŸá ⁄©ÿ≥€å ÿ™Ÿà ÿ±ÿß ÿ≥ÿßÿÆÿ™Ÿá',
		'who are you', 'who created you', 'who developed you', 'who wrote you',
		'where were you developed', 'where were you trained'
	]
	is_ai_question = any(keyword in req.text.lower() for keyword in ai_question_keywords)
	
	if is_ai_question:
		ai_response = """ŸÖŸÜ ÿØÿ≥ÿ™€åÿßÿ± ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å Ÿáÿ≥ÿ™ŸÖ ⁄©Ÿá ÿØÿ± ŸÖÿ±⁄©ÿ≤ ŸÖÿØ€åÿ±€åÿ™ Ÿà ÿ™ÿ≠ŸÑ€åŸÑ ÿØÿßÿØŸá ŸÅÿ±ÿßÿ¨ÿß Ÿà ÿßÿØÿßÿ±Ÿá ŸÖŸáŸÜÿØÿ≥€å ÿØÿßÿØŸá ÿ™Ÿàÿ≥ÿπŸá ÿØÿßÿØŸá ÿ¥ÿØŸá Ÿà ÿ¢ŸÖŸàÿ≤ÿ¥ ÿØ€åÿØŸá‚ÄåÿßŸÖ. ÿ™Ÿàÿ≥ÿπŸá‚ÄåÿØŸáŸÜÿØŸá ŸÖŸÜÿå ÿ≥ÿ±ŸáŸÜ⁄Ø ŸÖŸáŸÜÿØÿ≥ ÿπŸÑ€å ÿ≥ŸÑ€åŸÖ€å ÿßÿ≥ÿ™ Ÿà ÿ¢ŸÖÿßÿØŸá‚ÄåÿßŸÖ ÿ™ÿß ÿ¥ŸÖÿß ÿ±ÿß ÿ±ÿßŸáŸÜŸÖÿß€å€å ⁄©ŸÜŸÖ."""
		return MultiModelResponse(
			text=req.text,
			language=language,
			domain=domain,
			first_analysis=ModelAnalysis(
				entities=[{
					"text": ai_response,
					"label": "AI_Response",
					"start": 0,
					"end": len(ai_response),
					"confidence": 1.0
				}],
				relationships=[]
			),
			second_analysis=ModelAnalysis(
				entities=[{
					"text": ai_response,
					"label": "AI_Response",
					"start": 0,
					"end": len(ai_response),
					"confidence": 1.0
				}],
				relationships=[]
			),
			final_analysis=ModelAnalysis(
				entities=[{
					"text": ai_response,
					"label": "AI_Response",
					"start": 0,
					"end": len(ai_response),
					"confidence": 1.0
				}],
				relationships=[]
			),
			agreement_score=1.0,
			conflicting_entities=[],
			conflicting_relationships=[]
		)

	try:
		result = run_multi_model_analysis(
			text=req.text,
			language=language,
			domain=domain,
			model_first=req.model_first,
			model_second=req.model_second,
			model_referee=req.model_referee,
			temperature=temperature,
			max_output_tokens=max_output_tokens,
			request_timeout_seconds=config.REQUEST_TIMEOUT_SECONDS,
			num_ctx=config.NUM_CTX,
		)

		return MultiModelResponse(
			text=result["text"],
			language=result["language"],
			domain=result["domain"],
			first_analysis=ModelAnalysis(**result["first_analysis"]),
			second_analysis=ModelAnalysis(**result["second_analysis"]),
			final_analysis=ModelAnalysis(**result["final_analysis"]),
			agreement_score=result["agreement_score"],
			conflicting_entities=result["conflicting_entities"],
			conflicting_relationships=result["conflicting_relationships"],
		)
	except Exception as e:
		raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")


@app.post("/api/chat", response_model=ChatResponse)
def chat(req: ChatRequest) -> ChatResponse:
	"""Chat endpoint for conversational analysis with single/multi model support"""
	if not req.message or not req.message.strip():
		raise HTTPException(status_code=400, detail="'message' is required")

	language = (req.language or "fa").lower()
	domain = req.domain or "general"
	analysis_mode = req.analysisMode or "single"
	model_name = req.model or config.OLLAMA_MODEL

	try:
		# Build a conversational prompt based on domain
		domain_titles = {
			"police": "ÿØÿ≥ÿ™€åÿßÿ± ŸáŸàÿ¥ŸÖŸÜÿØ ÿßŸÖŸÜ€åÿ™€å Ÿà ŸæŸÑ€åÿ≥€å",
			"legal": "ÿØÿ≥ÿ™€åÿßÿ± ŸáŸàÿ¥ŸÖŸÜÿØ ÿ≠ŸÇŸàŸÇ€å", 
			"medical": "ÿØÿ≥ÿ™€åÿßÿ± ŸáŸàÿ¥ŸÖŸÜÿØ Ÿæÿ≤ÿ¥⁄©€å",
			"general": "ÿØÿ≥ÿ™€åÿßÿ± ŸáŸàÿ¥ŸÖŸÜÿØ ÿπŸÖŸàŸÖ€å"
		}
		
		domain_expertise = {
			"police": "ÿ¥ŸÖÿß ŸÖÿ™ÿÆÿµÿµ ÿ™ÿ≠ŸÑ€åŸÑ ŸÖÿ™ŸàŸÜ ÿßŸÖŸÜ€åÿ™€åÿå ŸæŸÑ€åÿ≥€åÿå ÿ¨ÿ±ÿß€åŸÖÿå ÿ™ŸáÿØ€åÿØÿßÿ™ Ÿà ŸÖŸàÿßÿ±ÿØ ŸÖÿ¥⁄©Ÿà⁄© Ÿáÿ≥ÿ™€åÿØ.",
			"legal": "ÿ¥ŸÖÿß ŸÖÿ™ÿÆÿµÿµ ÿ™ÿ≠ŸÑ€åŸÑ ŸÖÿ™ŸàŸÜ ÿ≠ŸÇŸàŸÇ€åÿå ŸÇŸàÿßŸÜ€åŸÜÿå ŸÇÿ±ÿßÿ±ÿØÿßÿØŸáÿßÿå ÿØÿßÿØ⁄ØÿßŸá‚ÄåŸáÿß Ÿà ŸÖÿ≥ÿßÿ¶ŸÑ ŸÇÿßŸÜŸàŸÜ€å Ÿáÿ≥ÿ™€åÿØ.",
			"medical": "ÿ¥ŸÖÿß ŸÖÿ™ÿÆÿµÿµ ÿ™ÿ≠ŸÑ€åŸÑ ŸÖÿ™ŸàŸÜ Ÿæÿ≤ÿ¥⁄©€åÿå ÿ™ÿ¥ÿÆ€åÿµ‚ÄåŸáÿßÿå ÿØÿ±ŸÖÿßŸÜ‚ÄåŸáÿßÿå ÿØÿßÿ±ŸàŸáÿß Ÿà ŸÖÿ≥ÿßÿ¶ŸÑ ÿ≥ŸÑÿßŸÖÿ™ Ÿáÿ≥ÿ™€åÿØ.",
			"general": "ÿ¥ŸÖÿß ŸÖÿ™ÿÆÿµÿµ ÿ™ÿ≠ŸÑ€åŸÑ ŸÖÿ™ŸàŸÜ ÿπŸÖŸàŸÖ€å Ÿà ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿ≥ÿßÿÆÿ™ÿßÿ±€åÿßŸÅÿ™Ÿá Ÿáÿ≥ÿ™€åÿØ."
		}
		
		title = domain_titles.get(domain, domain_titles["general"])
		expertise = domain_expertise.get(domain, domain_expertise["general"])
		
		analysis_mode_desc = "ÿ®ÿß ÿØÿßŸàÿ±€å ⁄ÜŸÜÿØŸÖÿØŸÑŸá" if analysis_mode == "multi" else ""
		
		# Enhanced system prompt that considers conversation history
		history_context = ""
		if req.message_history and len(req.message_history) > 0:
			# Analyze conversation history to provide better context
			user_messages = [msg.content for msg in req.message_history if msg.role == "user"]
			assistant_messages = [msg.content for msg in req.message_history if msg.role == "assistant"]
			
			# Extract key topics and names mentioned
			all_content = " ".join([msg.content for msg in req.message_history])
			key_topics = []
			
			# Look for names, places, and important topics
			import re
			name_patterns = [r'ŸÜÿßŸÖ\s+ŸÖŸÜ\s+(\w+)', r'ÿßÿ≥ŸÖ\s+ŸÖŸÜ\s+(\w+)', r'ŸÖŸÜ\s+(\w+)\s+Ÿáÿ≥ÿ™ŸÖ', r'(\w+)\s+Ÿáÿ≥ÿ™ŸÖ']
			for pattern in name_patterns:
				matches = re.findall(pattern, all_content, re.IGNORECASE)
				key_topics.extend(matches)
			
			# Look for important topics
			topic_keywords = ['ŸÖÿ¥⁄©ŸÑ', 'ÿÆÿ∑ÿß', 'ÿßÿ¥ÿ™ÿ®ÿßŸá', '⁄©ŸÖ⁄©', 'ÿ±ÿßŸáŸÜŸÖÿß€å€å', 'ÿ™ÿ≠ŸÑ€åŸÑ', 'ÿ®ÿ±ÿ±ÿ≥€å', 'ÿ≥ŸàÿßŸÑ', 'Ÿæÿßÿ≥ÿÆ', 'ÿ™Ÿàÿ∂€åÿ≠']
			for keyword in topic_keywords:
				if keyword in all_content.lower():
					key_topics.append(keyword)
			
			# Look for questions
			question_patterns = [r'(\w+)\s+⁄Ü€åŸá', r'(\w+)\s+⁄©€åÿ≥ÿ™', r'(\w+)\s+⁄©ÿ¨ÿßÿ≥ÿ™', r'⁄Üÿ∑Ÿàÿ±\s+(\w+)']
			for pattern in question_patterns:
				matches = re.findall(pattern, all_content, re.IGNORECASE)
				key_topics.extend(matches)
			
			topics_context = ""
			if key_topics:
				topics_context = f"\nŸÖŸàÿ∂Ÿàÿπÿßÿ™ ŸÖŸáŸÖ ÿØÿ± ŸÖ⁄©ÿßŸÑŸÖŸá: {', '.join(set(key_topics))}"
			
			# Analyze conversation length and complexity
			total_chars = sum(len(msg.content) for msg in req.message_history)
			avg_length = total_chars / len(req.message_history) if req.message_history else 0
			
			complexity_context = ""
			if len(req.message_history) > 10:
				complexity_context = "\nÿß€åŸÜ €å⁄© ŸÖ⁄©ÿßŸÑŸÖŸá ÿ∑ŸàŸÑÿßŸÜ€å ÿßÿ≥ÿ™. ŸÑÿ∑ŸÅÿßŸã ÿ≥ÿßÿ®ŸÇŸá ⁄©ÿßŸÖŸÑ ÿ±ÿß ÿØÿ± ŸÜÿ∏ÿ± ÿ®⁄Ø€åÿ±€åÿØ."
			elif avg_length > 100:
				complexity_context = "\nŸÖ⁄©ÿßŸÑŸÖŸá ÿ¥ÿßŸÖŸÑ Ÿæ€åÿßŸÖ‚ÄåŸáÿß€å ÿ∑ŸàŸÑÿßŸÜ€å ÿßÿ≥ÿ™. ŸÑÿ∑ŸÅÿßŸã ÿ¨ÿ≤ÿ¶€åÿßÿ™ ÿ±ÿß ÿØÿ± ŸÜÿ∏ÿ± ÿ®⁄Ø€åÿ±€åÿØ."
			
			history_context = f"""
ŸÖŸáŸÖ: ÿ¥ŸÖÿß ÿØÿ± ÿ≠ÿßŸÑ ÿßÿØÿßŸÖŸá €å⁄© ŸÖ⁄©ÿßŸÑŸÖŸá Ÿáÿ≥ÿ™€åÿØ. ŸÑÿ∑ŸÅÿßŸã ÿ≥ÿßÿ®ŸÇŸá ŸÖ⁄©ÿßŸÑŸÖŸá ŸÇÿ®ŸÑ€å ÿ±ÿß ÿØÿ± ŸÜÿ∏ÿ± ÿ®⁄Ø€åÿ±€åÿØ Ÿà Ÿæÿßÿ≥ÿÆ‚ÄåŸáÿß€å ÿÆŸàÿØ ÿ±ÿß ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ÿ¢ŸÜ ÿßÿ±ÿßÿ¶Ÿá ÿØŸá€åÿØ.

ÿØÿ≥ÿ™Ÿàÿ±ÿßŸÑÿπŸÖŸÑ‚ÄåŸáÿß€å ŸÖŸáŸÖ:
- ŸáŸÖ€åÿ¥Ÿá ÿ≥ÿßÿ®ŸÇŸá ŸÖ⁄©ÿßŸÑŸÖŸá ÿ±ÿß ÿ®ÿ±ÿ±ÿ≥€å ⁄©ŸÜ€åÿØ
- ÿß⁄Øÿ± ⁄©ÿßÿ±ÿ®ÿ± ÿ®Ÿá ŸÖŸàÿ∂Ÿàÿπÿßÿ™ ŸÇÿ®ŸÑ€å ÿßÿ¥ÿßÿ±Ÿá ŸÖ€å‚Äå⁄©ŸÜÿØÿå ÿßÿ≤ ÿ≥ÿßÿ®ŸÇŸá ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ€åÿØ
- Ÿæÿßÿ≥ÿÆ‚ÄåŸáÿß€å ÿÆŸàÿØ ÿ±ÿß ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ÿ≤ŸÖ€åŸÜŸá ŸÖ⁄©ÿßŸÑŸÖŸá ÿßÿ±ÿßÿ¶Ÿá ÿØŸá€åÿØ
- ÿß⁄Øÿ± ⁄©ÿßÿ±ÿ®ÿ± ÿ≥ŸàÿßŸÑ ÿ¨ÿØ€åÿØ€å ŸÖ€å‚ÄåŸæÿ±ÿ≥ÿØÿå ÿ¢ŸÜ ÿ±ÿß ÿØÿ± ÿßÿ±ÿ™ÿ®ÿßÿ∑ ÿ®ÿß ÿ≥ÿßÿ®ŸÇŸá Ÿæÿßÿ≥ÿÆ ÿØŸá€åÿØ

{topics_context}{complexity_context}

ÿ™ÿπÿØÿßÿØ Ÿæ€åÿßŸÖ‚ÄåŸáÿß€å ŸÇÿ®ŸÑ€å: {len(req.message_history)}
ÿ™Ÿàÿ¨Ÿá: ÿß€åŸÜ ŸÖ⁄©ÿßŸÑŸÖŸá ÿßÿØÿßŸÖŸá ÿØÿßÿ±ÿØÿå Ÿæÿ≥ ÿ≠ÿ™ŸÖÿßŸã ÿ≥ÿßÿ®ŸÇŸá ÿ±ÿß ÿØÿ± ŸÜÿ∏ÿ± ÿ®⁄Ø€åÿ±€åÿØ."""
		
		system_prompt = f"""ÿ¥ŸÖÿß {title} {analysis_mode_desc} ŸÖÿ±⁄©ÿ≤ ŸÖÿØ€åÿ±€åÿ™ Ÿà ÿ™ÿ≠ŸÑ€åŸÑ ÿØÿßÿØŸá ŸÅÿ±ÿßÿ¨ÿß Ÿáÿ≥ÿ™€åÿØ.
{expertise}

ŸÇŸàÿßŸÜ€åŸÜ ŸÖŸáŸÖ ÿ®ÿ±ÿß€å Ÿæÿßÿ≥ÿÆ‚ÄåÿØŸá€å:
1. ŸáŸÖ€åÿ¥Ÿá ÿ≥ÿßÿ®ŸÇŸá ŸÖ⁄©ÿßŸÑŸÖŸá ÿ±ÿß ÿØÿ± ŸÜÿ∏ÿ± ÿ®⁄Ø€åÿ±€åÿØ
2. ÿß⁄Øÿ± ⁄©ÿßÿ±ÿ®ÿ± ÿ®Ÿá ŸÖŸàÿ∂Ÿàÿπÿßÿ™ ŸÇÿ®ŸÑ€å ÿßÿ¥ÿßÿ±Ÿá ŸÖ€å‚Äå⁄©ŸÜÿØÿå ÿßÿ≤ ÿ≥ÿßÿ®ŸÇŸá ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ€åÿØ
3. Ÿæÿßÿ≥ÿÆ‚ÄåŸáÿß€å ÿÆŸàÿØ ÿ±ÿß ⁄©Ÿàÿ™ÿßŸáÿå ŸÖŸÅ€åÿØ Ÿà ÿ®Ÿá ÿ≤ÿ®ÿßŸÜ {language} ÿßÿ±ÿßÿ¶Ÿá ÿØŸá€åÿØ
4. ÿØÿ± ÿµŸàÿ±ÿ™ ŸÜ€åÿßÿ≤ ÿ™ÿ≠ŸÑ€åŸÑ ŸÖÿ™ŸÜ €åÿß ŸÜŸÖŸàÿØÿßÿ± ÿßÿ±ÿßÿ¶Ÿá ÿØŸá€åÿØ

ŸÇÿßÿ®ŸÑ€åÿ™‚ÄåŸáÿß€å Ÿà€å⁄òŸá ÿ¥ŸÖÿß:
- ÿ™ÿ≠ŸÑ€åŸÑ ŸÖÿ™ŸàŸÜ Ÿà ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ŸÖŸàÿ¨ŸàÿØ€åÿ™‚ÄåŸáÿß Ÿà ÿ±Ÿàÿßÿ®ÿ∑
- ⁄©ÿ¥€åÿØŸÜ ŸÜŸÖŸàÿØÿßÿ± ÿ®ÿ±ÿß€å ŸÜŸÖÿß€åÿ¥ ÿØÿßÿØŸá‚ÄåŸáÿß
- Ÿæÿßÿ≥ÿÆ‚ÄåÿØŸá€å ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ÿ≥ÿßÿ®ŸÇŸá ŸÖ⁄©ÿßŸÑŸÖŸá
- ÿßÿ±ÿ¨ÿßÿπ ÿ®Ÿá Ÿæ€åÿßŸÖ‚ÄåŸáÿß€å ŸÇÿ®ŸÑ€å

ÿ®ÿ±ÿß€å ⁄©ÿ¥€åÿØŸÜ ŸÜŸÖŸàÿØÿßÿ±ÿå ÿßÿ≤ ŸÅÿ±ŸÖÿ™ ÿ≤€åÿ± ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ€åÿØ:
```chart
{{
  "type": "bar|line|pie|doughnut",
  "title": "ÿπŸÜŸàÿßŸÜ ŸÜŸÖŸàÿØÿßÿ±",
  "labels": ["ÿ®ÿ±⁄Üÿ≥ÿ®1", "ÿ®ÿ±⁄Üÿ≥ÿ®2", "ÿ®ÿ±⁄Üÿ≥ÿ®3"],
  "datasets": [
    {{
      "label": "ŸÜÿßŸÖ ŸÖÿ¨ŸÖŸàÿπŸá ÿØÿßÿØŸá",
      "data": [10, 20, 30],
      "backgroundColor": ["#3b82f6", "#10b981", "#f59e0b"]
    }}
  ]
}}
```

{history_context}"""

		# Check if user is asking about the AI assistant
		ai_question_keywords = [
			'ÿ™Ÿà ⁄©€å Ÿáÿ≥ÿ™€å', 'ÿ™Ÿà ⁄©ÿ¨ÿß ÿ™Ÿàÿ≥ÿπŸá Ÿæ€åÿØÿß ⁄©ÿ±ÿØ€å', '⁄ÜŸá ⁄©ÿ≥€å ŸÜŸàÿ¥ÿ™Ÿá ÿßÿ™', '⁄ÜŸá ⁄©ÿ≥€å ÿ™Ÿàÿ≥ÿπŸá ÿØÿßÿØŸá ÿßÿ™',
			'⁄©ÿ¨ÿß ÿ¢ŸÖŸàÿ≤ÿ¥ ÿØ€åÿØŸá ÿß€å', 'ÿ™Ÿàÿ≥ÿπŸá ÿØŸáŸÜÿØŸá ÿ™Ÿà ⁄©€åÿ≥ÿ™', 'ŸÜŸà€åÿ≥ŸÜÿØŸá ÿ™Ÿà ⁄©€åÿ≥ÿ™', '⁄ÜŸá ⁄©ÿ≥€å ÿ™Ÿà ÿ±ÿß ÿ≥ÿßÿÆÿ™Ÿá',
			'who are you', 'who created you', 'who developed you', 'who wrote you',
			'where were you developed', 'where were you trained'
		]
		is_ai_question = any(keyword in req.message.lower() for keyword in ai_question_keywords)
		
		if is_ai_question:
			ai_response = """ŸÖŸÜ ÿØÿ≥ÿ™€åÿßÿ± ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å Ÿáÿ≥ÿ™ŸÖ ⁄©Ÿá ÿØÿ± ŸÖÿ±⁄©ÿ≤ ŸÖÿØ€åÿ±€åÿ™ Ÿà ÿ™ÿ≠ŸÑ€åŸÑ ÿØÿßÿØŸá ŸÅÿ±ÿßÿ¨ÿß Ÿà ÿßÿØÿßÿ±Ÿá ŸÖŸáŸÜÿØÿ≥€å ÿØÿßÿØŸá ÿ™Ÿàÿ≥ÿπŸá ÿØÿßÿØŸá ÿ¥ÿØŸá Ÿà ÿ¢ŸÖŸàÿ≤ÿ¥ ÿØ€åÿØŸá‚ÄåÿßŸÖ. ÿ™Ÿàÿ≥ÿπŸá‚ÄåÿØŸáŸÜÿØŸá ŸÖŸÜÿå ÿ≥ÿ±ŸáŸÜ⁄Ø ŸÖŸáŸÜÿØÿ≥ ÿπŸÑ€å ÿ≥ŸÑ€åŸÖ€å ÿßÿ≥ÿ™ Ÿà ÿ¢ŸÖÿßÿØŸá‚ÄåÿßŸÖ ÿ™ÿß ÿ¥ŸÖÿß ÿ±ÿß ÿ±ÿßŸáŸÜŸÖÿß€å€å ⁄©ŸÜŸÖ."""
			return ChatResponse(message=ai_response)
		
		# Check if user wants analysis
		analysis_keywords = ['ÿ™ÿ≠ŸÑ€åŸÑ', 'ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨', 'ŸÖŸàÿ¨ŸàÿØ€åÿ™', 'ÿ±ÿßÿ®ÿ∑Ÿá', 'analyze', 'extract', 'ÿ®ÿ±ÿ±ÿ≥€å', 'ÿ¥ŸÜÿßÿ≥ÿß€å€å']
		wants_analysis = any(keyword in req.message.lower() for keyword in analysis_keywords)

		if wants_analysis and len(req.message) > 50:
			# Perform analysis based on mode
			try:
				if analysis_mode == "multi" and req.model_first and req.model_second and req.model_referee:
					# Multi-model analysis
					analysis_result = run_multi_model_analysis(
						text=req.message,
						language=language,
						domain=domain,
						model_first=req.model_first,
						model_second=req.model_second,
						model_referee=req.model_referee,
						temperature=0.1,
						max_output_tokens=512,
						request_timeout_seconds=config.REQUEST_TIMEOUT_SECONDS,
						num_ctx=config.NUM_CTX,
					)
					
					entities_count = len(analysis_result["final_analysis"]["entities"])
					relationships_count = len(analysis_result["final_analysis"]["relationships"])
					agreement = analysis_result.get("agreement_score", 0) * 100
					conflicts = len(analysis_result.get("conflicting_entities", [])) + len(analysis_result.get("conflicting_relationships", []))
					
					response_message = f"""ŸÖÿ™ŸÜ ÿ¥ŸÖÿß ÿ®ÿß ÿØÿßŸàÿ±€å ⁄ÜŸÜÿØŸÖÿØŸÑŸá ÿ™ÿ≠ŸÑ€åŸÑ ÿ¥ÿØ:
‚öñÔ∏è ŸÜÿ™€åÿ¨Ÿá ŸÜŸáÿß€å€å ÿØÿßŸàÿ±:
- {entities_count} ŸÖŸàÿ¨ŸàÿØ€åÿ™ ÿ¥ŸÜÿßÿ≥ÿß€å€å ÿ¥ÿØ
- {relationships_count} ÿ±ÿßÿ®ÿ∑Ÿá €åÿßŸÅÿ™ ÿ¥ÿØ
- ŸÖ€åÿ≤ÿßŸÜ ÿ™ŸàÿßŸÅŸÇ ŸÖÿØŸÑ‚ÄåŸáÿß: {agreement:.1f}%
- ÿ™ÿπÿßÿ±ÿ∂ÿßÿ™ ÿ¥ŸÜÿßÿ≥ÿß€å€å ÿ¥ÿØŸá: {conflicts}

ÿ¨ÿ≤ÿ¶€åÿßÿ™ ⁄©ÿßŸÖŸÑ ÿØÿ± ŸæŸÜŸÑ ÿ≤€åÿ± ŸÜŸÖÿß€åÿ¥ ÿØÿßÿØŸá ÿ¥ÿØŸá ÿßÿ≥ÿ™."""

					# Convert to format expected by frontend
					formatted_result = {
						"text": analysis_result["text"],
						"language": analysis_result["language"],
						"domain": analysis_result["domain"],
						"first_analysis": analysis_result["first_analysis"],
						"second_analysis": analysis_result["second_analysis"],
						"final_analysis": analysis_result["final_analysis"],
						"agreement_score": analysis_result["agreement_score"],
						"conflicting_entities": analysis_result["conflicting_entities"],
						"conflicting_relationships": analysis_result["conflicting_relationships"],
					}

					return ChatResponse(
						message=response_message,
						analysis=formatted_result,
						analysisMode="multi"
					)
				else:
					# Single model analysis
					analysis_result = run_extraction(
						text=req.message,
						language=language,
						domain=domain,
						model=model_name,
						temperature=0.1,
						max_output_tokens=512
					)
					
					entities_count = len(analysis_result.get("entities", []))
					relationships_count = len(analysis_result.get("relationships", []))
					
					response_message = f"""ŸÖÿ™ŸÜ ÿ¥ŸÖÿß ÿ™ÿ≠ŸÑ€åŸÑ ÿ¥ÿØ:
- {entities_count} ŸÖŸàÿ¨ŸàÿØ€åÿ™ ÿ¥ŸÜÿßÿ≥ÿß€å€å ÿ¥ÿØ
- {relationships_count} ÿ±ÿßÿ®ÿ∑Ÿá €åÿßŸÅÿ™ ÿ¥ÿØ

ÿ¨ÿ≤ÿ¶€åÿßÿ™ ÿØÿ± ŸæŸÜŸÑ ÿ≤€åÿ± ŸÜŸÖÿß€åÿ¥ ÿØÿßÿØŸá ÿ¥ÿØŸá ÿßÿ≥ÿ™."""

					# Convert to format expected by frontend
					formatted_result = {
						"text": req.message,
						"language": language,
						"model": model_name,
						"entities": analysis_result.get("entities", []),
						"relationships": analysis_result.get("relationships", []),
					}

					return ChatResponse(
						message=response_message,
						analysis=formatted_result,
						analysisMode="single"
					)
			except Exception as e:
				print(f"Analysis failed: {str(e)}")
				pass  # Fall back to regular chat

		# Regular chat response with message history support
		
		# Convert message history to the format expected by ollama
		message_history = []
		if req.message_history:
			for msg in req.message_history:
				message_history.append({
					"role": msg.role,
					"content": msg.content
				})
			print(f"üìù Received message history: {len(message_history)} messages")
			for i, msg in enumerate(message_history):
				print(f"  {i+1}. {msg['role']}: {msg['content'][:50]}...")
			
			# Validate message history format
			if len(message_history) > 0:
				print(f"üìù First message: {message_history[0]['role']}: {message_history[0]['content'][:30]}...")
				print(f"üìù Last message: {message_history[-1]['role']}: {message_history[-1]['content'][:30]}...")
				
				# Analyze message patterns
				user_messages = [msg for msg in message_history if msg['role'] == 'user']
				assistant_messages = [msg for msg in message_history if msg['role'] == 'assistant']
				print(f"üìù Message analysis: {len(user_messages)} user, {len(assistant_messages)} assistant")
				
				# Check for important patterns
				all_content = " ".join([msg['content'] for msg in message_history])
				if 'ŸÜÿßŸÖ' in all_content or 'ÿßÿ≥ŸÖ' in all_content:
					print("üìù Contains name/identity information")
				if 'ŸÖÿ¥⁄©ŸÑ' in all_content or 'ÿÆÿ∑ÿß' in all_content:
					print("üìù Contains problem/error information")
				if 'ÿü' in all_content or '?' in all_content:
					print("üìù Contains questions")
		
		# Adjust parameters based on conversation complexity
		temperature = 0.7
		max_tokens = 256
		
		if req.message_history and len(req.message_history) > 10:
			# For long conversations, use lower temperature for consistency
			temperature = 0.5
			max_tokens = 512  # Allow longer responses for complex conversations
			print("üìù Adjusting parameters for long conversation")
		
		response = chat_conversational(
			system_prompt=system_prompt,
			user_message=req.message,
			model=model_name,
			message_history=message_history if message_history else None,
			temperature=temperature,
			max_output_tokens=max_tokens
		)

		# Clean up response
		clean_response = response.strip()
		if clean_response.startswith('"') and clean_response.endswith('"'):
			clean_response = clean_response[1:-1]
		
		# Extract chart data if present
		chart_data = None
		import re
		chart_pattern = r'```chart\s*\n(.*?)\n```'
		chart_match = re.search(chart_pattern, clean_response, re.DOTALL)
		if chart_match:
			try:
				import json
				chart_json = chart_match.group(1).strip()
				chart_data = json.loads(chart_json)
				# Remove chart block from response text
				clean_response = re.sub(chart_pattern, '', clean_response, flags=re.DOTALL).strip()
				print(f"üìä Chart data extracted: {chart_data.get('type', 'unknown')} chart")
			except json.JSONDecodeError as e:
				print(f"‚ö†Ô∏è Failed to parse chart JSON: {e}")
				chart_data = None

		# Log response quality
		print(f"üìù Response length: {len(clean_response)} characters")
		if len(clean_response) < 10:
			print("‚ö†Ô∏è Warning: Very short response")
		elif len(clean_response) > 500:
			print("üìù Long response generated")
		
		# Check if response seems to consider history
		if req.message_history and len(req.message_history) > 0:
			history_indicators = ['ŸÇÿ®ŸÑÿßŸã', 'ÿ≥ÿßÿ®ŸÇÿßŸã', '⁄ØŸÅÿ™ŸÖ', '⁄ØŸÅÿ™€åÿØ', 'ŸÇÿ®ŸÑ', 'Ÿæ€åÿ¥', 'ŸáŸÖÿßŸÜ', 'ŸáŸÖ€åŸÜ']
			considers_history = any(indicator in clean_response for indicator in history_indicators)
			if considers_history:
				print("‚úÖ Response appears to consider conversation history")
			else:
				print("‚ö†Ô∏è Response may not be considering conversation history")
			
			# Check for specific references to previous messages
			has_specific_reference = any(
				msg.content in clean_response or 
				(len(msg.content) > 10 and msg.content[:10] in clean_response)
				for msg in req.message_history
			)
			if has_specific_reference:
				print("‚úÖ Response contains specific reference to previous message")
			
			# Check for continuity in conversation
			has_continuity = any(
				msg.role == 'user' and msg.content.lower()[:5] in clean_response.lower()
				for msg in req.message_history
			)
			if has_continuity:
				print("‚úÖ Response shows good conversation continuity")
			
			# Check for context awareness
			has_context_awareness = any(
				msg.role == 'assistant' and msg.content.lower()[:5] in clean_response.lower()
				for msg in req.message_history
			)
			if has_context_awareness:
				print("‚úÖ Response shows context awareness from previous assistant messages")
			
			# Overall conversation quality assessment
			quality_indicators = [
				considers_history,
				has_specific_reference,
				has_continuity,
				has_context_awareness
			]
			quality_score = sum(quality_indicators)
			print(f"üìù Conversation quality score: {quality_score}/4")
			
			if quality_score >= 3:
				print("‚úÖ Excellent conversation quality")
			elif quality_score >= 2:
				print("‚úÖ Good conversation quality")
			elif quality_score >= 1:
				print("‚ö†Ô∏è Fair conversation quality")
			else:
				print("‚ö†Ô∏è Poor conversation quality - may not be considering history")
			
			# Store quality score for potential future use
			if quality_score < 2:
				print("‚ö†Ô∏è Consider improving conversation context or model parameters")
			
			# Log conversation summary for debugging
			print(f"üìù Conversation summary: {len(req.message_history)} messages, quality: {quality_score}/4")
			if req.message_history:
				last_user_message = next((msg for msg in reversed(req.message_history) if msg.role == 'user'), None)
				last_assistant_message = next((msg for msg in reversed(req.message_history) if msg.role == 'assistant'), None)
				if last_user_message:
					print(f"üìù Last user message: \"{last_user_message.content[:50]}...\"")
				if last_assistant_message:
					print(f"üìù Last assistant message: \"{last_assistant_message.content[:50]}...\"")
			
			# Log current message for context
			print(f"üìù Current user message: \"{req.message[:50]}...\"")
			print(f"üìù Current assistant response: \"{clean_response[:50]}...\"")
			
			# Log conversation flow for debugging
			print(f"üìù Conversation flow: {len(req.message_history)} previous messages ‚Üí current exchange")
			if req.message_history:
				user_messages = sum(1 for msg in req.message_history if msg.role == 'user')
				assistant_messages = sum(1 for msg in req.message_history if msg.role == 'assistant')
				print(f"üìù Previous flow: {user_messages} user messages, {assistant_messages} assistant messages")
			
			# Log conversation quality metrics
			print(f"üìù Quality metrics: History consideration: {considers_history}, Specific refs: {has_specific_reference}, Continuity: {has_continuity}, Context awareness: {has_context_awareness}")
			
			# Log conversation health check
			health_check = {
				'has_history': len(req.message_history) > 0,
				'has_quality': quality_score >= 2,
				'has_continuity': has_continuity,
				'has_context': has_context_awareness
			}
			print(f"üìù Conversation health check: {health_check}")
			
			if not health_check['has_history']:
				print("‚ö†Ô∏è No conversation history available")
			if not health_check['has_quality']:
				print("‚ö†Ô∏è Low conversation quality detected")
			
			# Log conversation improvement suggestions
			if quality_score < 2:
				print("üí° Suggestions for better conversation quality:")
				if not considers_history:
					print("  - Model may need better history context")
				if not has_specific_reference:
					print("  - Model may need to reference specific previous messages")
				if not has_continuity:
					print("  - Model may need better conversation continuity")
				if not has_context_awareness:
					print("  - Model may need better context awareness")

		return ChatResponse(message=clean_response, chart=chart_data)

	except Exception as e:
		raise HTTPException(status_code=500, detail=f"Chat failed: {str(e)}")


@app.post("/api/speech-to-text", response_model=SpeechToTextResponse)
async def speech_to_text(
	audio_file: UploadFile = File(...),
	language: str = Form("fa"),
	whisper_model_size: str = Form("base")
) -> SpeechToTextResponse:
	"""Convert speech to text using the speech-to-text microservice"""
	try:
		# Prepare form data for the speech-to-text service
		files = {"audio_file": (audio_file.filename, await audio_file.read(), audio_file.content_type)}
		data = {"language": language, "model_size": whisper_model_size}
		
		# Call the speech-to-text microservice
		async with httpx.AsyncClient(timeout=60.0) as client:
			response = await client.post(
				"http://localhost:8001/transcribe",
				files=files,
				data=data
			)
			
			if response.status_code != 200:
				raise HTTPException(
					status_code=response.status_code,
					detail=f"Speech-to-text service error: {response.text}"
				)
			
			result = response.json()
			return SpeechToTextResponse(
				text=result["text"],
				language=result["language"],
				confidence=result.get("confidence")
			)
			
	except httpx.RequestError as e:
		raise HTTPException(status_code=503, detail=f"Speech-to-text service unavailable: {str(e)}")
	except Exception as e:
		raise HTTPException(status_code=500, detail=f"Speech-to-text failed: {str(e)}")
